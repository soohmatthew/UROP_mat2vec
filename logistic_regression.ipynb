{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Please ensure that you are in the proper working directory\n"
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'regex'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-ceed65e79a51>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./UROP_mat2vec'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMaterialsTextProcessor\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[0mtext_processor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMaterialsTextProcessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\Dell\\OneDrive\\Documents\\School\\Year 4 Sem 1 (SMU)\\UROP\\UROP_mat2vec\\utils\\process.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mregex\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0munidecode\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mos\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpath\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmonty\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfractions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgcd_float\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'regex'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import ast\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "if not os.getcwd().endswith('UROP_mat2vec'):\n",
    "    print('Please ensure that you are in the proper working directory')\n",
    "\n",
    "os.chdir('./UROP_mat2vec')\n",
    "\n",
    "from utils.process import MaterialsTextProcessor\n",
    "text_processor = MaterialsTextProcessor()\n",
    "\n",
    "\n",
    "\n",
    "with open('data/relevant_articles_material_science.json') as json_file:\n",
    "    relevant = json.load(json_file)\n",
    "\n",
    "relevant_articles = ast.literal_eval(relevant)\n",
    "relevant_df = pd.DataFrame({'abstracts' : relevant_articles})\n",
    "\n",
    "with open('data/irrelevant_articles.json') as json_file:\n",
    "    irrelevant = json.load(json_file)\n",
    "    \n",
    "irrelevant_articles = [i for i in irrelevant if i != '']\n",
    "\n",
    "for fname in os.listdir('.'):\n",
    "    if 'irrelevant_articles_' in fname:\n",
    "        with open('data/' + fname) as json_file:\n",
    "            irrelevant_articles += ast.literal_eval(json.load(json_file))\n",
    "            \n",
    "irrelevant_df = pd.DataFrame({'abstracts' : irrelevant_articles})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process Abstracts\n",
    "\n",
    "1. Remove foreign language\n",
    "2. Remove common words like \"Abstracts\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = set(nltk.corpus.words.words())\n",
    "\n",
    "def language_checker(string):\n",
    "    remove_foreign = \" \".join(w for w in nltk.wordpunct_tokenize(string) if w.lower() in words or not w.isalpha())\n",
    "    # If more than 30% of words are cut, remove article, else keep\n",
    "    if len(remove_foreign)/len(string) <= 0.5:\n",
    "\n",
    "        return \"\"\n",
    "    else:\n",
    "        return string\n",
    "    \n",
    "def remove_weblinks(string):\n",
    "    return re.sub('https?://[A-Za-z0-9./]+','', string)\n",
    "    \n",
    "relevant_df = relevant_df[relevant_df['abstracts'].apply(language_checker) != \"\"]\n",
    "irrelevant_df = irrelevant_df[irrelevant_df['abstracts'].apply(language_checker) != \"\"]\n",
    "relevant_df = relevant_df[relevant_df['abstracts'].apply(remove_weblinks) != \"\"]\n",
    "irrelevant_df = irrelevant_df[irrelevant_df['abstracts'].apply(remove_weblinks) != \"\"]\n",
    "irrelevant_df['abstracts'] = irrelevant_df['abstracts'].apply(lambda x: x.replace(\"Background\", \"\"))\n",
    "irrelevant_df['abstracts'] = irrelevant_df['abstracts'].apply(lambda x: x.replace(\"Abstract\", \"\"))\n",
    "relevant_df['abstracts'] = relevant_df['abstracts'].apply(lambda x: x.replace(\"Background\", \"\"))\n",
    "relevant_df['abstracts'] = relevant_df['abstracts'].apply(lambda x: x.replace(\"Abstract\", \"\"))\n",
    "relevant_df['label'] = 1\n",
    "irrelevant_df['label'] = 0\n",
    "df = pd.concat([relevant_df, irrelevant_df])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x = df.abstracts\n",
    "y = df.label\n",
    "\n",
    "SEED = 2000\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=.3, random_state=SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(733,)\n",
      "(3198,)\n",
      "(302,)\n",
      "(1384,)\n"
     ]
    }
   ],
   "source": [
    "print(y_train[y_train == 1].shape)\n",
    "print(y_train[y_train == 0].shape)\n",
    "\n",
    "print(y_test[y_test == 1].shape)\n",
    "print(y_test[y_test == 0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Logistic Regression',\n",
       "  0.994661921708185,\n",
       "  0.9848739495798319,\n",
       "  5.244019031524658),\n",
       " ('Linear SVC', 0.998220640569395, 0.9950083194675541, 5.576784372329712),\n",
       " ('LinearSVC with L1-based feature selection',\n",
       "  0.9887307236061684,\n",
       "  0.9675213675213675,\n",
       "  4.818987131118774),\n",
       " ('SVC', 0.8208778173190985, 0.0, 19.786147832870483),\n",
       " ('Multinomial NB', 0.9359430604982206, 0.848314606741573, 3.764889717102051),\n",
       " ('Bernoulli NB', 0.6441281138790036, 0.5016611295681063, 3.5783746242523193),\n",
       " ('Ridge Classifier',\n",
       "  0.9976275207591934,\n",
       "  0.9933554817275748,\n",
       "  4.219410181045532),\n",
       " ('AdaBoost', 0.9976275207591934, 0.9933554817275748, 8.682260274887085),\n",
       " ('Nearest Centroid',\n",
       "  0.9578884934756821,\n",
       "  0.8945022288261515,\n",
       "  7.156768560409546),\n",
       " ('XGBoost', 0.99644128113879, 0.9899665551839464, 19.569392681121826)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.linear_model import RidgeClassifier, LogisticRegression\n",
    "from sklearn.neighbors import NearestCentroid\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.pipeline import Pipeline\n",
    "from time import time\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "names = [\"Logistic Regression\", \"Linear SVC\", \"LinearSVC with L1-based feature selection\", \"Multinomial NB\", \n",
    "         \"Bernoulli NB\", \"Ridge Classifier\", \"AdaBoost\", \"Nearest Centroid\", \"XGBoost\"]\n",
    "classifiers = [LogisticRegression(),\n",
    "    LinearSVC(),\n",
    "    Pipeline([('feature_selection', SelectFromModel(LinearSVC(penalty=\"l1\", dual=False))), ('classification', LinearSVC(penalty=\"l2\"))]),\n",
    "    MultinomialNB(),\n",
    "    BernoulliNB(),\n",
    "    RidgeClassifier(),\n",
    "    AdaBoostClassifier(),\n",
    "    NearestCentroid(), \n",
    "    XGBClassifier(random_state=1,learning_rate=0.01)]\n",
    "\n",
    "zipped_clf = zip(names,classifiers)\n",
    "\n",
    "tvec = TfidfVectorizer()\n",
    "def accuracy_summary(pipeline, x_train, y_train, x_test, y_test):\n",
    "    if len(x_test[y_test == 0]) / (len(x_test)*1.) > 0.5:\n",
    "        null_accuracy = len(x_test[y_test == 0]) / (len(x_test)*1.)\n",
    "    else:\n",
    "        null_accuracy = 1. - (len(x_test[y_test == 0]) / (len(x_test)*1.))\n",
    "    t0 = time()\n",
    "    model = pipeline.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_test)\n",
    "    train_test_time = time() - t0\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, labels = np.unique(y_pred))\n",
    "    return accuracy, f1, train_test_time\n",
    "\n",
    "def classifier_comparator(vectorizer=tvec, n_features=10000, stop_words=None, ngram_range=(1, 1), classifier=zipped_clf):\n",
    "    result = []\n",
    "    vectorizer.set_params(stop_words=stop_words, max_features=n_features, ngram_range=ngram_range)\n",
    "    for n,c in classifier:\n",
    "        checker_pipeline = Pipeline([('vectorizer', vectorizer),\n",
    "                                     ('classifier', c)])\n",
    "        clf_accuracy, f1_score, tt_time = accuracy_summary(checker_pipeline, x_train, y_train, x_test, y_test)\n",
    "        result.append((n, clf_accuracy, f1_score, tt_time))\n",
    "    return result\n",
    "\n",
    "trigram_result = classifier_comparator(n_features=20000 ,ngram_range=(1,3))\n",
    "\n",
    "trigram_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9780033840947547"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([('vectorizer', TfidfVectorizer()), ('classifier', LogisticRegression())])\n",
    "model = pipeline.fit(x_train, y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(model.predict(relevant_df['abstracts']))\n",
    "sum(model.predict(irrelevant_df['abstracts']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['CoLiO2', 'is', 'a', 'battery', 'cathode', 'material', '.'],\n",
       " [('LiCoO2', 'CoLiO2')])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_processor = MaterialsTextProcessor()\n",
    "text_processor.process(\"LiCoO2 is a battery cathode material.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Dell\\\\OneDrive\\\\Documents\\\\School\\\\Year 4 Sem 1 (SMU)\\\\UROP\\\\Logistic Regression'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}